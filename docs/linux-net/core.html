<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html lang="en"><head><title>Memo: Linux Networking</title>
<meta http-equiv="Expires" content="Thu, 21 Oct 2004 05:46:47 +0000">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="description" content="Linux Networking">
<meta name="generator" content="xml2rfc v1.25 (http://xml.resource.org/)">
<style type="text/css">
<!--
    body {
        font-family: verdana, charcoal, helvetica, arial, sans-serif;
        margin: 2em;
        font-size: small ; color: #000000 ; background-color: #ffffff ; }
    .title { color: #990000; font-size: x-large ;
        font-weight: bold; text-align: right;
        font-family: helvetica, monaco, "MS Sans Serif", arial, sans-serif;
        background-color: transparent; }
    .filename { color: #666666; font-size: 18px; line-height: 28px;
        font-weight: bold; text-align: right;
        font-family: helvetica, arial, sans-serif;
        background-color: transparent; }
    td.rfcbug { background-color: #000000 ; width: 30px ; height: 30px ; 
        text-align: justify; vertical-align: middle ; padding-top: 2px ; }
    td.rfcbug span.RFC { color: #666666; font-weight: bold; text-decoration: none;
        background-color: #000000 ;
        font-family: monaco, charcoal, geneva, "MS Sans Serif", helvetica, verdana, sans-serif;
        font-size: x-small ; }
    td.rfcbug span.hotText { color: #ffffff; font-weight: normal; text-decoration: none;
        text-align: center ;
        font-family: charcoal, monaco, geneva, "MS Sans Serif", helvetica, verdana, sans-serif;
        font-size: x-small ; background-color: #000000; }
/* info code from SantaKlauss at http://www.madaboutstyle.com/tooltip2.html */
    div#counter{margin-top: 100px}

    a.info{
        position:relative; /*this is the key*/
        z-index:24;
        text-decoration:none}

    a.info:hover{z-index:25; background-color:#990000 ; color: #ffffff ;}

    a.info span{display: none}

    a.info:hover span{ /*the span will display just on :hover state*/
        display:block;
        position:absolute;
        font-size: smaller ;
        top:2em; left:2em; width:15em;
        padding: 2px ;
        border:1px solid #333333;
        background-color:#eeeeee; color:#990000;
        text-align: left ;}

     A { font-weight: bold; }
     A:link { color: #990000; background-color: transparent ; }
     A:visited { color: #333333; background-color: transparent ; }
     A:active { color: #333333; background-color: transparent ; }

    p { margin-left: 2em; margin-right: 2em; }
    p.copyright { font-size: x-small ; }
    p.toc { font-size: small ; font-weight: bold ; margin-left: 3em ;}

    span.emph { font-style: italic; }
    span.strong { font-weight: bold; }
    span.verb { font-family: "Courier New", Courier, monospace ; }

    ol.text { margin-left: 2em; margin-right: 2em; }
    ul.text { margin-left: 2em; margin-right: 2em; }
    li { margin-left: 3em;  }

    pre { margin-left: 3em; color: #333333;  background-color: transparent;
        font-family: "Courier New", Courier, monospace ; font-size: small ;
        }

    h3 { color: #333333; font-size: medium ;
        font-family: helvetica, arial, sans-serif ;
        background-color: transparent; }
    h4 { font-size: small; font-family: helvetica, arial, sans-serif ; }

    table.bug { width: 30px ; height: 15px ; }
    td.bug { color: #ffffff ; background-color: #990000 ;
        text-align: center ; width: 30px ; height: 15px ;
         }
    td.bug A.link2 { color: #ffffff ; font-weight: bold;
        text-decoration: none;
        font-family: monaco, charcoal, geneva, "MS Sans Serif", helvetica, sans-serif;
        font-size: x-small ; background-color: transparent }

    td.header { color: #ffffff; font-size: x-small ;
        font-family: arial, helvetica, sans-serif; vertical-align: top;
        background-color: #666666 ; width: 33% ; }
    td.author { font-weight: bold; margin-left: 4em; font-size: x-small ; }
    td.author-text { font-size: x-small; }
    table.data { vertical-align: top ; border-collapse: collapse ;
        border-style: solid solid solid solid ;
        border-color: black black black black ;
        font-size: small ; text-align: center ; }
    table.data th { font-weight: bold ;
        border-style: solid solid solid solid ;
        border-color: black black black black ; }
    table.data td {
        border-style: solid solid solid solid ;
        border-color: #333333 #333333 #333333 #333333 ; }

    hr { height: 1px }
-->
</style></head>
<body>
<table summary="layout" class="bug" align="right" cellpadding="0" cellspacing="2"><tbody><tr><td class="bug"><a href="#toc" class="link2">&nbsp;TOC&nbsp;</a></td></tr></tbody></table>
<table summary="layout" border="0" cellpadding="0" cellspacing="0" width="66%"><tbody><tr><td><table summary="layout" border="0" cellpadding="2" cellspacing="1" width="100%">
<tbody><tr><td class="header">Memo</td><td class="header">A. Shankar</td></tr>
<tr><td class="header">&nbsp;</td><td class="header">September 2004</td></tr>
</tbody></table></td></tr></tbody></table>
<div align="right"><span class="title"><br>Linux Networking</span></div>
<a name="toc"></a><br><hr>
<h3>Table of Contents</h3>
<p class="toc">
<a href="#introduction">1.</a>&nbsp;
Introduction<br>
<a href="#receive">2.</a>&nbsp;
Receive<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#anchor1">2.1</a>&nbsp;
Background<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#anchor2">2.1.1</a>&nbsp;
Direct Memory Access (DMA)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#anchor3">2.1.2</a>&nbsp;
Interrupt Livelock<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#softirqs">2.1.3</a>&nbsp;
Softirqs<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#anchor4">2.2</a>&nbsp;
NAPI<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#anchor5">2.2.1</a>&nbsp;
Handling a flood<br>
<a href="#transmission">3.</a>&nbsp;
Transmission<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#anchor6">3.1</a>&nbsp;
The device and it's driver<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#anchor7">3.2</a>&nbsp;
Queues and Disciplines<br>
<a href="#conclusions">4.</a>&nbsp;
Conclusions<br>
<a href="#questions">5.</a>&nbsp;
More Questions<br>
<a href="#rfc.references1">6.</a>&nbsp;
References<br>
<a href="#rfc.authors">ยง</a>&nbsp;
Author's Address<br>
</p>
<br clear="all">

<a name="introduction"></a><br><hr>
<table summary="layout" class="bug" align="right" cellpadding="0" cellspacing="2"><tbody><tr><td class="bug"><a href="#toc" class="link2">&nbsp;TOC&nbsp;</a></td></tr></tbody></table>
<a name="rfc.section.1"></a><h3>1.&nbsp;Introduction</h3>

<p>This document presents my understanding of the implementation and
performance issues of software routing. The questions I want to answer
include:
</p>
<p></p>
<ul class="text">
<li>How many threads are involved in the networking stack?
</li>
<li>How long does a received packet spend in the system without
	being processed?
</li>
<li>How long does a packet to be sent wait before going out on
	the interface?
</li>
</ul>

<p>Due to easy access to documentation and source, focus is on the the Linux
implementation.
</p>
<p>We start by tracing the lifeline of a received packet, followed it by the
lifeline of a packet to be transmitted and finally combine two to answer the
questions above.
</p>
<p>The focus is on the networking layer and below. TCP/UDP is not looked at
(in particular, the TCP implementation will have some other thread(s) for
handling retransmissions etc. We ignore those for the moment).
</p>
<a name="receive"></a><br><hr>
<table summary="layout" class="bug" align="right" cellpadding="0" cellspacing="2"><tbody><tr><td class="bug"><a href="#toc" class="link2">&nbsp;TOC&nbsp;</a></td></tr></tbody></table>
<a name="rfc.section.2"></a><h3>2.&nbsp;Receive</h3>

<a name="rfc.section.2.1"></a><h4><a name="anchor1">2.1</a>&nbsp;Background</h4>

<p>A very high-level view is that packet reception causes the network
	interface device to raise an interrupt, in response to which the
	interrupt handler copies the packet from the device to main memory
	and then processes it (looks at headers etc.). Packet handling itself
	should (and is) not done in the interrupt context (i.e., when some
	interrupts may be masked), but done upon return from the interrupt
	context (i.e., when all interrupts are enabled). Linux uses "softirqs"
	for this, details of which are described in <a class="info" href="#softirqs">Section 2.1.3<span>Softirqs</span></a>.
</p>
<a name="rfc.section.2.1.1"></a><h4><a name="anchor2">2.1.1</a>&nbsp;Direct Memory Access (DMA)</h4>

<p>There are some bottlenecks in this high-level view. For
		example, copying the packet from the device's buffers to main
		memory. Instead, the device can use DMA to copy the packet
		into memory and then interrupt the processor only when the
		packet is "accessible". This is precisely what happens, each
		device driver maintains a DMA ring (circular buffer) and the
		device interrupts the processor only when the packet is ready
		in memory.
</p>
<p>On interface initialization (i.e., when it is brought "up"
		which means when the driver's "open()" function is called),
		the driver sets up and initializes a DMA ring buffer. When a
		packet is received, it is copied into main memory (one of the
		slots in the DMA ring buffer) and an interrupt is raised only
		after the packet is accessible to the kernel.
</p>
<a name="rfc.section.2.1.2"></a><h4><a name="anchor3">2.1.2</a>&nbsp;Interrupt Livelock</h4>

<p>Another problem is what is commonly referred to as a
		"livelock". If packets are being received fast enough, the
		kernel never gets to process them because interrupts are being
		generated too fast, i.e., the CPU spends 100% of its time in
		interrupt handling. A "fix" to this problem (and some others)
		was described in <a class="info" href="#napi">[2]<span>Salim, J., Olsson, R. and A. Kuznetsov, Beyond Softnet, November 2001.</span></a> and is referred to as
		NAPI (New API).  The changes suggested in this paper were
		included in Linux 2.4.20 and to the best of my knowledge, this
		is what happens in kernels between 2.4.20 and 2.6.8. The rest
		of this section thus talks about Linux networking code using
		NAPI. Note that not all drivers use the NAPI interface, for
		example in 2.4.23 it seems only the tg3 driver uses NAPI.
		Kernel 2.6.6 has more NAPI drivers such as b44, 8139cp and
		typhoon (source in drivers/net). Older (non-NAPI) drivers are
		still supported and there is a backward compatibility "hack",
		but we will not be concerned with the old methodology (called
		softnet).
</p>
<a name="rfc.section.2.1.3"></a><h4><a name="softirqs">2.1.3</a>&nbsp;Softirqs</h4>

<p>In order to keep the kernel response time small, interrupts
		must be disabled for as little time as possible. To achieve
		this, the tasks that need to be done upon an interrupt are
		divided into two - critical or "top-half" and "deferrable".
		Deferrable tasks execute with all interrupts enabled.
</p>
<p>Softirqs is how Linux implements deferrable tasks. (This
		section is essentially a summary of Chapter 4 in <a class="info" href="#book">[3]<span>Bovet, D. and M. Cesati, Understanding the Linux Kernel, .</span></a>). There are a limited number of softirqs (the
		kernel code allows for 32 but only 4 seem to be used. These
		different softirqs are called "types"). The properties of a
		softirq are:
</p>
<p></p>
<ul class="text">
<li>No softirq can be interrupted to run another softirq
			on the same CPU
</li>
<li>Softirqs (of any type) can run concurrently on
			multiple CPUs
</li>
</ul>

<p>When a network interface interrupts on packet reception, a
		softirq is scheduled. The softirq code is then executed when
		the interrupt handler finishes.
</p>
<p>The softirq code type for received network packets is
		NET_RX_SOFTIRQ and the handling code is in the function
		net_rx_action().
</p>
<p>Typically, the sequence of events is something like
		this:
</p>
<p></p>
<ol class="text">
<li>device generates interrupt
</li>
<li>interrupt handler schedules softirq
</li>
<li>interrupt handler exits (and now all interrupts are
		enabled)
</li>
<li>a check is made for all scheduled softirqs (remember that
		there are other softirqs besides NET_RX_SOFTIRQ) and their
		handlers are called. This is done by do_softirq()
</li>
</ol>

<p>This background should be enough for now, some finer
		details will be discussed later.
</p>
<a name="rfc.section.2.2"></a><h4><a name="anchor4">2.2</a>&nbsp;NAPI</h4>

<p>Here we describe some of details of <a class="info" href="#napi">NAPI<span>Salim, J., Olsson, R. and A. Kuznetsov, Beyond Softnet, November 2001.</span></a>[2]. One of the primary goals of NAPI was
		to prevent livelocks and it does so by "adaptive interrupt
		coalescing".  The idea is to use a mixture of interrupts and
		polling - i.e., at times device interrupts are disabled and
		packets are collected by polling instead. This reduces the
		chances of a livelock.
</p>
<p>As mentioned earlier, the model of DMA was that the network
		interface device would interrupt the processor when the packet
		has been copied into main memory. A per-packet or
		per-fixed-number-of-packets interrupt helps in creating
		livelocks. NAPI proposes the following change: the device
		interrupts the processor upon reception of the "first" packet.
		The interrupt handler adds the device to a poll list, letting
		the kernel know that there is some work to be done on the
		device. The driver then disables further interrupts caused by
		a new incoming packet or by running out of buffers.  Clearly,
		this practice reduces the number of interrupts generated by
		the device. It also leads to silent packet drops upon signs of
		overload (when the buffers are full), which is another design
		goal of NAPI.
</p>
<p>The situation at this point is this - the kernel knows that
		there is a device with "some" packets on it. The device itself
		will not trouble with interrupts as more packets arrive.
</p>
<p>When the softirq is activated, all devices that registered
		themselves are polled for packets. Note that the list of
		devices to be polled is a per-CPU list. So each CPU has a
		mutually exclusive set of devices to poll.
</p>
<p>The polling functionality itself is implemented by the
		driver (poll function pointer in struct net_device). The
		driver's poll() function is called by net_rx_action() asking
		it to process received packets and setting a limit on the
		total number of packets that it can process (to ensure
		fairness amongst interfaces). If all outstanding packets have
		been processed, then the driver re-enables receive interrupts.
		Earlier we mentioned that upon receipt of the "first" packet,
		the driver disables receive-related interrupts from the
		device. What this really means is that if a receive generates
		an interrupt then it will generate no more until explicitly
		re-enabled. Interrupts will be re-enabled only after the kernel
		gets around to processing the packets already received.
</p>
<p>Documentation included with the Linux kernel source<a class="info" href="#kernel-doc-napi_howto">[4]<span>, Linux kernel source: 			Documentation/networking/NAPI_HOWTO.txt, .</span></a> describes the NAPI interface
		in greater detail.
</p>
<p>The net effect of NAPI is that under low loads it converges
		to an interrupt driven scheme (where "low load" gives the
		kernel enough time to process a packet before the next one
		arrives) and under high load the ratio of interrupts to
		packets is reduced. Results in <a class="info" href="#napi">[2]<span>Salim, J., Olsson, R. and A. Kuznetsov, Beyond Softnet, November 2001.</span></a> show how
		the scheme reduces latency and increases throughput (ratio of
		outgoing packets to incoming packets in a packet forwarder).
		Although we have not discussed this here, NAPI also prevents
		packet re-ordering which was possible with the pre-NAPI
		(softnet) kernels.
</p>
<p>Thus, NAPI limits the interruption rate (this can be seen as
		adaptive interrupt coalescing) and prevents livelocks.
		However, since a device is handled by at most one CPU at a
		time, it does limit the parallelism possible with a single
		interface.
</p>
<a name="rfc.section.2.2.1"></a><h4><a name="anchor5">2.2.1</a>&nbsp;Handling a flood</h4>

<p>Suppose there is a flood of incoming packets. NAPI's
		adaptive interrupt coalescing prevents a flood of interrupts.
		If packets are coming faster than the softirq can process
		them, i.e., the device's DMA ring is full, then the device
		silently drops the new packets.
</p>
<p>So far so good. The kernel seems to be processing packets
		as fast as it can. However, remember that softirqs (and thus
		net_rx_action(), the driver's poll() function etc.) are
		invoked upon return from an interrupt handler. With this
		infrastructure, the kernel will just try to keep up with
		packet processing and user level code will never get to the
		CPU. Thus, user level processes, if any, will starve.
</p>
<p>This is where a special kernel thread (ksoftirqd_CPUn)
		comes in, one thread for each CPU. Let's look at how softirqs
		come into being again. When interrupt handling finishes (the
		do_IRQ() function) then do_softirq() is called. This function
		simply looks at pending softirq's and calls their handlers
		(NET_RX_SOFTIRQ is a softirq and net_rx_action() is it's
		corresponding handler). The do_softirq() function ensures
		that it calls a particular softirq handler at most once. For
		example, after net_rx_action() returns if do_softirq() notices
		that there is another NET_RX_SOFTIRQ pending then it does NOT
		call net_rx_action() again. Instead it wakes up a low priority
		thread - ksoftirqd. ksoftirqd runs in an infinite loop,
		checking if there is any pending softirq and if so executes
		it's handler.
</p>
<p>The net_rx_action() handler processes at most
		netdev_max_backlog packets in one invocation. In kernels
		2.4.23 and 2.6.8 for example, netdev_max_backlog is set to 300
		by default. This value can be changed using
		/proc/sys/net/core/netdev_max_backlog. If there are more
		packets to be handled, then net_rx_action() schedules another
		NET_RX_SOFTIRQ. The softirq schedules itself.
</p>
<p>Thus, when net_rx_action() returns after handling 300 or so
		packets, do_softirq() notices that NET_RX_ACTION has been
		scheduled again. Instead of trying to process those packets
		it wakes up ksoftirqd. Now, the remaining packets will
		be processed when ksoftirqd gets CPU time.
</p>
<p>ksoftirqd is a low priority thread, so if there are user
		processes waiting for CPU time, they will get it before
		ksoftirqd. This mechanism ensures that under heavy network
		traffic, users of a system don't get frustrated. On the other
		hand, if the system acts as a router then there should not be
		many user processes wanting CPU time and hence ksoftirqd runs
		all the time.
</p>
<p>In short, under heavy traffic, all seems under control. The
		kernel tries to keep up with the incoming packet rate without
		starving user processes or being interrupted by the hardware
		too much.
</p>
<a name="transmission"></a><br><hr>
<table summary="layout" class="bug" align="right" cellpadding="0" cellspacing="2"><tbody><tr><td class="bug"><a href="#toc" class="link2">&nbsp;TOC&nbsp;</a></td></tr></tbody></table>
<a name="rfc.section.3"></a><h3>3.&nbsp;Transmission</h3>

<p>Packet transmission seems to be much simple that reception. In this section
we take a bottom-up approach - starting from the device we move up to how the
network layer manages queues of packets to be sent.
</p>
<a name="rfc.section.3.1"></a><h4><a name="anchor6">3.1</a>&nbsp;The device and it's driver</h4>

<p>DMA during transmission works as a "streaming" mapping instead of a
	"consistent" mapping (see Chapter 13 in <a class="info" href="#ldd">[5]<span>Rubini, A. and J. Corbet, Linux Device Drivers, .</span></a>). In the
	case of the receive buffers, the driver can provide the device with a
	fixed set of addresses where it should place incoming packets, thus
	the DMA mapping is fixed for the lifetime of the driver (it is 
	"consistent"). This works because the number of incoming packets is
	limited by the device. A system can however generate packets for
	transmission faster than the device can handle. Hence, when the device
	driver is given a packet to send, it first sets up a DMA mapping (mapping
	on a per-operation basis is called "streaming") and then instructs the
	hardware to start sending.
</p>
<p>The transmission routine simply sets up the DMA mapping and some
	registers on the hardware and exits. It does not wait for the device
	to physically complete transmission. Concurrent accesses to the
	driver's transmission function (hard_start_xmit()) are prevented by a
	spinlock (xmit_lock).
</p>
<p>The device can only handle a limited number of outstanding packets.
	Thus, calls to the transmission function are throttled - when the
	transmission ring is full, no more calls to the driver's transmission
	function are made. The "queue" of packets is "stopped"
	(netif_stop_queue()). When the device is ready for more, then the queue
	is woken up (netif_wake_queue()) and packets in the queue can be sent.
	The NET_TX_SOFTIRQ softirq is used to achieve this. When the packet
	transmission code sees that the device's queue is stopped, it
	schedules a NET_TX_SOFTIRQ. As in the case of the receive path, the
	ksoftirqd thread plays a role here.
</p>
<p>The story so far is thus - the device driver provides a function
	which instructs the hardware to begin transmission. This function is
	protected from concurrent accesses by a spinlock. Under heavy load
	(when the hardware is unable to keep up), packet transmission may be
	deferred to the low priority ksoftirqd thread.
</p>
<p>The device driver also has a timeout mechanism to react to
	occasional hardware faults (such as losing an interrupt). For this the
	driver provides a timeout callback function and the kernel implements
	a watchdog timer (dev_watchdog()). However, we will not concern
	ourselves with the timeouts at this time.
</p>
<a name="rfc.section.3.2"></a><h4><a name="anchor7">3.2</a>&nbsp;Queues and Disciplines</h4>

<p>So far we have seen what happens when the device driver is
	instructed to send a packet. But how to packets get to this stage?
</p>
<p>Let's consider a packet ready to send, present in the network (IP)
	layer. It could have reached here either because of higher layers
	(TCP/UDP) or it is a packet that was received and needs to be
	forwarded. Some higher level protocols such as UDP/ICMP are fairly
	simple and the packet reaches the IP layer (ip_queue_xmit()) almost
	"immediately", i.e., only a few function calls lie in between. TCP on
	the other hand is slightly more complicated with its congestion
	window, retransmission timers etc. So we ignore TCP for now and see
	virtually no latency/delay between the time the packet was created or
	a decision was made to forward it and the time the IP layer sees it.
	(NOTE: The exact sequence of function calls may be slightly tedious to
	trace, but looks something like this: ip_queue_xmit() --&gt; ip_output()
	--&gt; ip_finish_output() --&gt; dev_queue_xmit()).
</p>
<p>The link layer is where some complexity sets in because the device
	driver may not be invoked to send out the packet immediately, instead
	the packet may be queued for later transmission.
</p>
<p>Each device maintains a queue of packets to be transmitted. How a
	packet is selected from this queue is known as the "queuing
	discipline" (and causes many variables and functions in the Linux
	kernel to be named "qdisc..."). The default qdisc is FIFO, but Linux
	allows other sophisticated schemes such as RED (Random Early
	Detection?), CBQ and others.
</p>
<p>When the link layer transmission function (dev_queue_xmit()) is
	called, it enqueues the packet according to the queuing discipline. It
	then dequeues a packet (the queueing discipline will select the "best"
	packet to send now) and invokes the device driver's transmit function
	(dev_queue_xmit() --&gt; qdisc_run() --&gt; qdisc_restart() --&gt;
	dev-&gt;hard_start_xmit()). In case of any problems, such as the device
	being locked by a thread on another CPU (the xmit_lock spinlock) or
	the device not being in a position to accept more packets, a softirq
	(NET_TX_SOFTIRQ) is scheduled and packet transmission will happen at a
	later time.
</p>
<p>Packet transmission will be tried again when the softirq is run
	(ksoftirqd thread) or when there is an attempt to send another packet
	and the device is free at that time.
</p>
<p>In short, packets to be sent get queued and are sent out "over the
	wire" by the ksoftirqd thread if immediate sending is not possible.
</p>
<a name="conclusions"></a><br><hr>
<table summary="layout" class="bug" align="right" cellpadding="0" cellspacing="2"><tbody><tr><td class="bug"><a href="#toc" class="link2">&nbsp;TOC&nbsp;</a></td></tr></tbody></table>
<a name="rfc.section.4"></a><h3>4.&nbsp;Conclusions</h3>

<p>Based on the discussion so far, we now look try to answer the questions
posed at the beginning in <a class="info" href="#introduction">Section 1<span>Introduction</span></a>.
</p>
<p></p>
<ul class="text">
<li>How many threads are involved in the networking stack (IP and
	below)? - A per-CPU ksoftirqd thread and a per-interface watchdog
	timer thread. 
	
	If the packet rate is very low, then the threads do not get involved:
	receive handling happens just right after the interrupt handler returns
	and transmission happens immediately. If the rates are very high, then
	most receive processing and sends are handled by the ksoftirqd thread.
	The watchdog timer just restarts transmissions in case there were some
	hardware errors.
</li>
<li>How long does a received packet spend in the system without being
	processed? - Assuming there are no other processes to run, processing
	of received packets is delayed only by interrupts and packet
	transmissions. With NAPI interrupt coalescing, receive interrupts are
	limited and occur only when the kernel is ready to receive and process
	more. Packet transmissions on the other hand may cause many
	interrupts. Will have to investigate further to find out how much
	interference these interrupts cause. Remember that packet
	transmissions are throttled depending on the size of the send buffer
	of the device.
</li>
<li>How long does a packet to be sent wait before going out on the
	interface? - Packet transmissions are throttled based on the number of
	outstanding transmissions the device can handle. Thus, packet
	transmissions are limited by the device. When the device is ready to
	send more packets it will generate an interrupt and the next packet to
	be sent will at worst have to wait for some received packets to be
	processed (netdev_max_backlog packets at most). This is because packet
	transmissions and receive processing are both done by a softirq and
	two softirqs cannot run on the same CPU at the same time.
</li>
</ul>

<a name="questions"></a><br><hr>
<table summary="layout" class="bug" align="right" cellpadding="0" cellspacing="2"><tbody><tr><td class="bug"><a href="#toc" class="link2">&nbsp;TOC&nbsp;</a></td></tr></tbody></table>
<a name="rfc.section.5"></a><h3>5.&nbsp;More Questions</h3>

<p>Some questions on the "TODO" list:
</p>
<p></p>
<ul class="text">
<li>NAPI can save on tons of receive interrupts. But for transmission,
	there is still one interrupt per successful send? If so, in a
	forwarder, the number and rate of incoming and outgoing packets should
	be nearly equal and if livelocks were caused due to per-packet receive
	interrupts then don't per-packet transmission related interrupts have
	an adverse affect as well?
</li>
<li>Some more details on the transmission watchdog timer. How
	frequently do retransmissions need to take place?
</li>
</ul>

<a name="rfc.references1"></a><br><hr>
<table summary="layout" class="bug" align="right" cellpadding="0" cellspacing="2"><tbody><tr><td class="bug"><a href="#toc" class="link2">&nbsp;TOC&nbsp;</a></td></tr></tbody></table>
<h3>6&nbsp;References</h3>
<table border="0" width="99%">
<tbody><tr><td class="author-text" valign="top"><a name="linuxmap">[1]</a></td>
<td class="author-text">Rio, M., Goutelle, M., Kelly, T., Hughes-Jones, R., Martin-Flatin, J. and Y. Li, "<a href="http://datatag.web.cern.ch/datatag/papers/tr-datatag-2004-1.pdf">A Map of the Networking Code in Linux Kernel
			2.4.20</a>", March 2004.</td></tr>
<tr><td class="author-text" valign="top"><a name="napi">[2]</a></td>
<td class="author-text">Salim, J., Olsson, R. and A. Kuznetsov, "<a href="http://www.usenix.org/publications/library/proceedings/als01/jamal.html">Beyond Softnet</a>", November 2001.</td></tr>
<tr><td class="author-text" valign="top"><a name="book">[3]</a></td>
<td class="author-text">Bovet, D. and M. Cesati, "Understanding the Linux Kernel", ISBN 81-7366-589-3, Publisher O'Reilly Associates. 2nd Edition.</td></tr>
<tr><td class="author-text" valign="top"><a name="kernel-doc-napi_howto">[4]</a></td>
<td class="author-text">"Linux kernel source:
			Documentation/networking/NAPI_HOWTO.txt".</td></tr>
<tr><td class="author-text" valign="top"><a name="ldd">[5]</a></td>
<td class="author-text">Rubini, A. and J. Corbet, "<a href="http://www.xml.com/ldd/chapter/book/">Linux Device Drivers</a>", ISBN 0-59600-008-1, Publisher O'Reilly Associates. 2nd Edition. June 2001.</td></tr>
</tbody></table>

<a name="rfc.authors"></a><br><hr>
<table summary="layout" class="bug" align="right" cellpadding="0" cellspacing="2"><tbody><tr><td class="bug"><a href="#toc" class="link2">&nbsp;TOC&nbsp;</a></td></tr></tbody></table>
<h3>Author's Address</h3>
<table border="0" cellpadding="0" cellspacing="0" width="99%">
<tbody><tr><td class="author-text">&nbsp;</td>
<td class="author-text">Asim Shankar</td></tr>
<tr><td class="author" align="right">EMail:&nbsp;</td>
<td class="author-text"><a href="mailto:shankar@uiuc.edu">shankar@uiuc.edu</a></td></tr>
</tbody></table>
</body></html>